之前写过一篇文章，讲贝叶斯定理的定义和应用。今天，我们来具体完成中文拼写纠正的功能。
![design]({{ site.url }}/assets/20170829/google.png)
# 一.原理
用户输入一个中文词语，可能拼写正确，也可能错误。我们用c(correct)表示拼写正确,w(wrong)表示拼写错误。拼写纠正就是要在拼写错误的情况下，推测出可能性最大的正确的拼写。即
>P(c|w)

根据贝叶斯定理，
>P(c|w)=P(c)*P(w|c)/P(w)

在错误拼写确定的情况下，P(w)是一定的。所以使得
>P(c)*P(w|c)

最大的那个词c，就是我们要求的最有可能的正确的拼写。其中P(c)正比于词c的频率;P(w|c)大小和词c，w的相似度有关，如果词c和词w相似，那么P(w|c)会比较大。
# 二.算法
```
输入 w
IF w是一个字 OR w在字典中存在
    则 返回w
ELSE
    查找和w相似的词->similars
    IF similars为空
        则 返回w
    ELSE
        返回similars中最大频率的那个词c
```
# 三.代码实现
我们使用python来编写代码。
### 1.准备词库和词频
这里我使用的是jieba词库[dict.txt.big](https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/dict.txt.big)。词库文件中每一行是一个条目，其中包含词，词频，词性三个信息，以空格分开。

下面是加载词库和词频的代码。
```Python
def loadWords(filename):
    """Load words from word file.
    
    Args:
        filename:Name of the word source.
    
    Returns:
        A dict mapping word's length to words and frequences tuple.
        For examples:
    
        {2: [("资源",3),("你好",4)],
         4: [("一心一意",2),("三心二意",3)]}
    """ 
    wordmap={}
    with open(filename) as f:
        for line in f:
            wt=line.split(' ')
            w=wt[0].decode("utf-8")
            wl = len(w)
            wordList=wordmap.get(wl)
            if wordList == None:
                wordList = []
                wordmap[wl]=wordList
            wordList.append((w,int(wt[1])))
    return wordmap


WORDMAP=loadWords('dict.txt.big')
```
### 2.判断词是否在词库中存在
```Python
def exist(word):
    """Check if word exists in word source.
    
    Returns:
        True if word exists,False otherwise.  
    """ 
    ws=WORDMAP.get(len(word))
    if ws == None:
        return False
    for (w,f) in ws: 
        if w == word:
            return True
    return False
```
### 3.查找相似词
如果两个词有一半或者一半以上的字相同，就认为两个词相似。代码如下。
```Python
def similarWords(word):
    """Get words that is half similar with the given word.
    
    Returns:
        Tuples of word and frequence.
    """ 
    candidates=[]
    ws=WORDMAP.get(len(word))
    if ws==None:
        return candidates
    halfLen=(len(word)+1)/2
    for (w,f) in ws: 
        count=0
        for c in word:
            if c in w:
                count=count+1
        if count >= halfLen:
            candidates.append((w,f))
    return candidates
```
### 4.选出最有可能正确的词
```Python
def correct(word):
    """Get the most likely correct word for the given word.
    If the given word exists in data source or it's length is one,return itself.
    If the given word has half similar words,return the similar
    word with max frequence.
    Otherwise return itself.

    Returns:
        A word.
    """
    word=word.decode("utf-8")
    if len(word)<=1 or exist(word):
        return word
    wfp=similarWords(word)
    if wfp == None or len(wfp) == 0:
        return word
    wr=None
    maxfre=0
    for (w,f) in wfp:
        if f > maxfre:
            wr=w
            maxfre=f
    return wr
```
至此代码完成。
# 四.改进方案
大概测试了下，上面程序的准确率只有20%，只能够在"自我消遣"级别玩玩。我对以上方案做了分析，发现几个比较严重的失误。
### 1.查找相似词的算法有问题
平时使用电子设备输入汉字，大多是通过拼音输入法输入的。如果输入的词有错误，很可能是因为拼音写错了。所以如果按照词的拼音相似度查找相似词，准确率应该能提高到40%左右。
### 2.词库太小了
词库太小了，很多词是正确的但是在词库中查不到。还有很多新出现的词，也查不到。如果能有个千万级别的词库，再加上新词发现功能发现的新词，准确率应该能提高到70%左右。
这些是我个人的想法，没有实践验证。有感兴趣的可以试试。
